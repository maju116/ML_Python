{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0018a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "tf.__version__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced0a7d4",
   "metadata": {},
   "source": [
    "Autoenkodery mogą być używane do wielu różnych rzeczy, takich jak segmentacja obrazu, redukcja wymiarowości, wykrywanie anomalii i wiele innych. Dzisiaj zaczniemy od prostego autoenkodera odszumiającego. Użyjemy zestawu danych MNIST dostępnego w Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6412215",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28, 28, 1) / 255\n",
    "x_test = x_test.reshape(-1, 28, 28, 1) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2beec",
   "metadata": {},
   "source": [
    "W zadaniu odszumiania wprowadzamy do autoenkodera oryginalne obrazy (danye) z dodanym szumem. Wyjście to po prostu oryginalny obraz (dane). Chcemy zminimalizować **błąd rekonstrukcji** między zaszumionym a oryginalnym obrazem. Aby to zrobić, musimy najpierw utworzyć zestaw danych pociągu zaszumionego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_noise = np.clip(x_train + np.random.normal(0, 0.5, x_train.shape), 0, 1)\n",
    "x_test_noise = np.clip(x_test + np.random.normal(0, 0.5, x_test.shape), 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83023b1",
   "metadata": {},
   "source": [
    "Teraz możemy zbudować prosty autoenkoder odszumiający:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainiclalizuj model\n",
    "autoencoder = ___\n",
    "# Encoder part\n",
    "# Dodaj warstweę konwolucji 2D: 32 filtry wielkości 3x3, aktywacja relu, padding \"same\"\n",
    "___\n",
    "# Dodaj warstwę Max pooling 2D: 2x2, padding \"same\"\n",
    "___\n",
    "# Dodaj warstweę konwolucji 2D: 32 filtry wielkości 3x3, aktywacja relu, padding \"same\"\n",
    "___\n",
    "# Dodaj warstwę Max pooling 2D: 2x2, padding \"same\"\n",
    "___\n",
    "# Decoder part\n",
    "# Dodaj warstweę konwolucji 2D: 32 filtry wielkości 3x3, aktywacja relu, padding \"same\"\n",
    "___\n",
    "# Dodaj warstwę UpSamplingu 2D: size 2x2\n",
    "___\n",
    "# Dodaj warstweę konwolucji 2D: 32 filtry wielkości 3x3, aktywacja relu, padding \"same\"\n",
    "___\n",
    "# Dodaj warstwę UpSamplingu 2D: size 2x2\n",
    "___\n",
    "# Dodaj warstweę konwolucji 2D: 32 filtry wielkości 3x3, aktywacja relu, padding \"same\"\n",
    "___\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578f0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skompiluj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113af5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wytrenuj model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ced22",
   "metadata": {},
   "source": [
    "Zobaczmy prognozy dotyczące zestawu testowego z szumami:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_predictions = autoencoder.predict(x_test_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e844b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "print(\"Original:\")\n",
    "plt.matshow(x_test[i,:, :, 0])\n",
    "print(\"Noisy:\")\n",
    "plt.matshow(x_test_noise[i,:, :, 0])\n",
    "print(\"Reconstructed:\")\n",
    "plt.matshow(autoencoder_predictions[i,:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db884c1",
   "metadata": {},
   "source": [
    "Z powyższego przykładu widzieliście, że w zadaniu oznaczającym byliśmy zainteresowani wyjściem **dekodera** (rekonstrukcja), teraz użyjemy autoenkoderów do redukcji wymiarowości. Tym razem będziemy zainteresowani wyjściem **enkodera** (reprezentacja niskowymiarowa). Po raz kolejny wykorzystamy zbiór danych MNIST. Tym razem chcemy zbudować autoenkoder oparty na MLP, więc będziemy musieli przekształcić dane w wektory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec = x_train.reshape((60000, 784))\n",
    "x_test_vec = x_test.reshape((10000, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65dfc4d",
   "metadata": {},
   "source": [
    "Tym razem nie możemy po prostu skorzystać z modelu sekwencyjnego, będziemy musieli skorzystać z funkcjonalnego API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ed24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stwórz Input do API funkcyjnego\n",
    "inputs = ___\n",
    "\n",
    "# Dodaj warstwę głęboką: 128 neuronów, aktywacja relu\n",
    "encoder = ___\n",
    "# Dodaj warstwę głęboką: 64 neurony, aktywacja relu\n",
    "encoder = ___\n",
    "# Dodaj warstwę głęboką: 32 neurony, aktywacja relu\n",
    "encoder = ___\n",
    "\n",
    "# Dodaj warstwę głęboką: 64 neurony, aktywacja relu\n",
    "decoder = ___\n",
    "# Dodaj warstwę głęboką: 128 neuronów, aktywacja relu\n",
    "decoder = ___\n",
    "# Dodaj warstwę głęboką: 784 neuronów, aktywacja relu\n",
    "decoder = ___\n",
    "\n",
    "# Stwórz model przy pomocy API funkcyjnego\n",
    "autoencoder = ___\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fedaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.Model(inputs, encoder)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f4f62",
   "metadata": {},
   "source": [
    "Skompilujemy model z `mse` jako funkcją straty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939181e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skompiluj model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d19801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wytrenuj model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fba9d",
   "metadata": {},
   "source": [
    "Teraz możemy sprawdzić rekonstrukcje i reprezentacje niskowymiarowe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1431e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_predictions = autoencoder.predict(x_test_vec)\n",
    "encoder_predictions = encoder.predict(x_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbae28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print(\"Original:\")\n",
    "plt.matshow(x_test_vec[i,:].reshape((28, 28)))\n",
    "print(\"Low-dim:\")\n",
    "plt.matshow(encoder_predictions[i,:].reshape((1, 32)))\n",
    "print(\"Reconstructed:\")\n",
    "plt.matshow(autoencoder_predictions[i,:].reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_predictions[i,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512158f",
   "metadata": {},
   "source": [
    "W ostatnim zadaniu użyjemy autoenkoderów do wykrywania anomalii. Będziemy korzystać ze zbioru danych dotyczących oszustw kredytowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fc8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_train_X = pd.read_csv(\"data/creditcard_train_X\", sep=\" \").to_numpy()\n",
    "creditcard_test_X = pd.read_csv(\"data/creditcard_test_X\", sep=\" \").to_numpy()\n",
    "creditcard_train_Y = pd.read_csv(\"data/creditcard_train_Y\", sep=\" \").to_numpy()\n",
    "creditcard_test_Y = pd.read_csv(\"data/creditcard_test_Y\", sep=\" \").to_numpy()\n",
    "\n",
    "print(creditcard_train_X.shape)\n",
    "print(creditcard_train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79e21b",
   "metadata": {},
   "source": [
    "W zadaniu wykrywania anomalii interesuje nas znalezienie próbek (transakcji), które mają duży błąd rekonstrukcji, co może nam powiedzieć, że jest w nich coś niezwykłego. W tym zadaniu możemy użyć modelu sekwencyjnego podobnie jak w zadaniu odszumiającym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zainicjalizuj model\n",
    "autoencoder = ___\n",
    "# Dodaj warstwę głęboką: 14 neuronów, aktywacja tanh\n",
    "___\n",
    "# Dodaj warstwę głęboką: 7 neuronów, aktywacja relu\n",
    "___\n",
    "# Dodaj warstwę głęboką: 7 neuronów, aktywacja tanh\n",
    "___\n",
    "# Dodaj warstwę głęboką: 29 neuronów, aktywacja relu\n",
    "___\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da65be9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skompiluj model\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wytrenuj model\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5168322e",
   "metadata": {},
   "source": [
    "Teraz możemy obliczyć błąd rekonstrukcji zestawu testowego i znaleźć najlepszy punkt odcięcia dla oszustwa związanego z kartą kredytową:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(creditcard_test_X)\n",
    "reconstruction_error = ((creditcard_test_X - predictions)**2).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'reconstruction_error': reconstruction_error,\n",
    "    'fraud': creditcard_test_Y[:, 0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.hist(column = 'reconstruction_error', by = 'fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f7825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, thresholds = precision_recall_curve(results.fraud, results.reconstruction_error)\n",
    "alpha = 0.004\n",
    "denom = alpha/precision + (1-alpha)/recall\n",
    "f_scores = np.divide(1, denom, out=np.zeros_like(denom), where=(denom!=0))\n",
    "max_f = np.max(f_scores)\n",
    "max_f_thresh = thresholds[np.argmax(f_scores)]\n",
    "\n",
    "pd.crosstab(results.fraud, results.reconstruction_error >= max_f_thresh,\n",
    "           rownames = [\"true\"], colnames = [\"predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f_thresh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
