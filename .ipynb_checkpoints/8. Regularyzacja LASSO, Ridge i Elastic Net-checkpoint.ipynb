{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88fa4fb1",
   "metadata": {},
   "source": [
    "# Regularyzacja LASSO, Ridge, Elastic Net\n",
    "\n",
    "## Przedstawienie problemu\n",
    "\n",
    "Ponizsze dane zawierają informację na temat współczynnika genetycznego pewnej choroby oraz ekspresji genów dla $1000$ pacjentów. Naszym zadaniem będzie sprawdzenie zależności (zdudowanie modelu statystycznego) pomiędzy informacją genetyczną, a indeksem chorobowym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca2f6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease_indicator</th>\n",
       "      <th>gene_1</th>\n",
       "      <th>gene_2</th>\n",
       "      <th>gene_3</th>\n",
       "      <th>gene_4</th>\n",
       "      <th>gene_5</th>\n",
       "      <th>gene_6</th>\n",
       "      <th>gene_7</th>\n",
       "      <th>gene_8</th>\n",
       "      <th>gene_9</th>\n",
       "      <th>...</th>\n",
       "      <th>gene_4991</th>\n",
       "      <th>gene_4992</th>\n",
       "      <th>gene_4993</th>\n",
       "      <th>gene_4994</th>\n",
       "      <th>gene_4995</th>\n",
       "      <th>gene_4996</th>\n",
       "      <th>gene_4997</th>\n",
       "      <th>gene_4998</th>\n",
       "      <th>gene_4999</th>\n",
       "      <th>gene_5000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.937794</td>\n",
       "      <td>-1.207066</td>\n",
       "      <td>-1.205333</td>\n",
       "      <td>-0.973819</td>\n",
       "      <td>-0.385072</td>\n",
       "      <td>-0.962873</td>\n",
       "      <td>-0.825144</td>\n",
       "      <td>0.023392</td>\n",
       "      <td>0.249881</td>\n",
       "      <td>-0.299325</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.303823</td>\n",
       "      <td>-1.997002</td>\n",
       "      <td>-1.268515</td>\n",
       "      <td>-0.166643</td>\n",
       "      <td>0.173221</td>\n",
       "      <td>1.027971</td>\n",
       "      <td>0.299238</td>\n",
       "      <td>-0.077520</td>\n",
       "      <td>1.866126</td>\n",
       "      <td>0.217892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.110904</td>\n",
       "      <td>0.277429</td>\n",
       "      <td>0.301467</td>\n",
       "      <td>-0.099631</td>\n",
       "      <td>0.514055</td>\n",
       "      <td>-0.813712</td>\n",
       "      <td>0.347168</td>\n",
       "      <td>-0.204902</td>\n",
       "      <td>0.481920</td>\n",
       "      <td>-0.362398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.581623</td>\n",
       "      <td>-0.732298</td>\n",
       "      <td>-1.348586</td>\n",
       "      <td>0.372561</td>\n",
       "      <td>-0.765171</td>\n",
       "      <td>0.190115</td>\n",
       "      <td>-0.048095</td>\n",
       "      <td>-0.098347</td>\n",
       "      <td>-0.277120</td>\n",
       "      <td>-0.048143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.342600</td>\n",
       "      <td>1.084441</td>\n",
       "      <td>-1.539145</td>\n",
       "      <td>-0.110735</td>\n",
       "      <td>0.308004</td>\n",
       "      <td>-0.194512</td>\n",
       "      <td>-0.920093</td>\n",
       "      <td>-1.049402</td>\n",
       "      <td>-0.584051</td>\n",
       "      <td>-0.924871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461320</td>\n",
       "      <td>-0.400628</td>\n",
       "      <td>0.187157</td>\n",
       "      <td>0.267106</td>\n",
       "      <td>0.677449</td>\n",
       "      <td>1.074012</td>\n",
       "      <td>0.047437</td>\n",
       "      <td>0.935116</td>\n",
       "      <td>-0.628793</td>\n",
       "      <td>-0.764516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.747371</td>\n",
       "      <td>-2.345698</td>\n",
       "      <td>0.635371</td>\n",
       "      <td>1.192195</td>\n",
       "      <td>1.839154</td>\n",
       "      <td>1.916013</td>\n",
       "      <td>-0.287336</td>\n",
       "      <td>0.016889</td>\n",
       "      <td>1.042713</td>\n",
       "      <td>0.395760</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.727101</td>\n",
       "      <td>-0.270288</td>\n",
       "      <td>1.228877</td>\n",
       "      <td>-2.670715</td>\n",
       "      <td>-0.519727</td>\n",
       "      <td>-0.328582</td>\n",
       "      <td>-0.825309</td>\n",
       "      <td>-0.817777</td>\n",
       "      <td>-0.716570</td>\n",
       "      <td>-0.224503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.872028</td>\n",
       "      <td>0.429125</td>\n",
       "      <td>0.702952</td>\n",
       "      <td>-1.655886</td>\n",
       "      <td>1.593405</td>\n",
       "      <td>0.701769</td>\n",
       "      <td>-0.551130</td>\n",
       "      <td>-0.306343</td>\n",
       "      <td>1.384544</td>\n",
       "      <td>0.281415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587151</td>\n",
       "      <td>1.441910</td>\n",
       "      <td>-1.574249</td>\n",
       "      <td>-1.514545</td>\n",
       "      <td>0.830039</td>\n",
       "      <td>-0.379623</td>\n",
       "      <td>0.531506</td>\n",
       "      <td>0.906511</td>\n",
       "      <td>1.072289</td>\n",
       "      <td>0.666093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4.415095</td>\n",
       "      <td>-0.216320</td>\n",
       "      <td>0.261722</td>\n",
       "      <td>1.706471</td>\n",
       "      <td>0.555580</td>\n",
       "      <td>-1.196431</td>\n",
       "      <td>-0.355663</td>\n",
       "      <td>-1.966713</td>\n",
       "      <td>0.788707</td>\n",
       "      <td>0.330298</td>\n",
       "      <td>...</td>\n",
       "      <td>1.753508</td>\n",
       "      <td>1.066614</td>\n",
       "      <td>2.494430</td>\n",
       "      <td>1.558894</td>\n",
       "      <td>0.936826</td>\n",
       "      <td>-0.339432</td>\n",
       "      <td>1.196886</td>\n",
       "      <td>0.639897</td>\n",
       "      <td>1.187620</td>\n",
       "      <td>-0.401922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>5.078293</td>\n",
       "      <td>0.549856</td>\n",
       "      <td>1.334695</td>\n",
       "      <td>-1.521243</td>\n",
       "      <td>0.192257</td>\n",
       "      <td>0.141452</td>\n",
       "      <td>0.227806</td>\n",
       "      <td>0.710682</td>\n",
       "      <td>0.607903</td>\n",
       "      <td>0.902859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058202</td>\n",
       "      <td>-1.711519</td>\n",
       "      <td>0.063803</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>1.855251</td>\n",
       "      <td>0.760492</td>\n",
       "      <td>0.570959</td>\n",
       "      <td>-0.437103</td>\n",
       "      <td>0.205727</td>\n",
       "      <td>-0.674824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-2.644617</td>\n",
       "      <td>0.482736</td>\n",
       "      <td>0.247747</td>\n",
       "      <td>1.146917</td>\n",
       "      <td>1.026826</td>\n",
       "      <td>-0.393832</td>\n",
       "      <td>-0.561344</td>\n",
       "      <td>-0.376735</td>\n",
       "      <td>-0.734039</td>\n",
       "      <td>-0.426135</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.151355</td>\n",
       "      <td>-1.563203</td>\n",
       "      <td>0.631759</td>\n",
       "      <td>-0.040236</td>\n",
       "      <td>1.014956</td>\n",
       "      <td>-0.695214</td>\n",
       "      <td>0.123247</td>\n",
       "      <td>0.391909</td>\n",
       "      <td>0.735752</td>\n",
       "      <td>-0.157513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>5.212235</td>\n",
       "      <td>0.760849</td>\n",
       "      <td>-1.375785</td>\n",
       "      <td>0.234440</td>\n",
       "      <td>-0.983478</td>\n",
       "      <td>-0.061958</td>\n",
       "      <td>-0.371238</td>\n",
       "      <td>0.289356</td>\n",
       "      <td>2.841597</td>\n",
       "      <td>0.731189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986182</td>\n",
       "      <td>-0.189626</td>\n",
       "      <td>1.366594</td>\n",
       "      <td>0.174719</td>\n",
       "      <td>0.899885</td>\n",
       "      <td>-1.272737</td>\n",
       "      <td>0.970013</td>\n",
       "      <td>-0.255478</td>\n",
       "      <td>-0.880389</td>\n",
       "      <td>-1.138162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-1.854773</td>\n",
       "      <td>-0.445658</td>\n",
       "      <td>-1.190540</td>\n",
       "      <td>0.543219</td>\n",
       "      <td>-0.932695</td>\n",
       "      <td>-0.287801</td>\n",
       "      <td>-0.228249</td>\n",
       "      <td>-0.575495</td>\n",
       "      <td>0.448721</td>\n",
       "      <td>0.162182</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.376273</td>\n",
       "      <td>-0.178267</td>\n",
       "      <td>0.788993</td>\n",
       "      <td>-0.828447</td>\n",
       "      <td>-0.478302</td>\n",
       "      <td>0.499964</td>\n",
       "      <td>0.758177</td>\n",
       "      <td>-1.500024</td>\n",
       "      <td>0.600020</td>\n",
       "      <td>-0.108616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     disease_indicator    gene_1    gene_2    gene_3    gene_4    gene_5  \\\n",
       "0            -2.937794 -1.207066 -1.205333 -0.973819 -0.385072 -0.962873   \n",
       "1             1.110904  0.277429  0.301467 -0.099631  0.514055 -0.813712   \n",
       "2            -7.342600  1.084441 -1.539145 -0.110735  0.308004 -0.194512   \n",
       "3             7.747371 -2.345698  0.635371  1.192195  1.839154  1.916013   \n",
       "4            -2.872028  0.429125  0.702952 -1.655886  1.593405  0.701769   \n",
       "..                 ...       ...       ...       ...       ...       ...   \n",
       "995           4.415095 -0.216320  0.261722  1.706471  0.555580 -1.196431   \n",
       "996           5.078293  0.549856  1.334695 -1.521243  0.192257  0.141452   \n",
       "997          -2.644617  0.482736  0.247747  1.146917  1.026826 -0.393832   \n",
       "998           5.212235  0.760849 -1.375785  0.234440 -0.983478 -0.061958   \n",
       "999          -1.854773 -0.445658 -1.190540  0.543219 -0.932695 -0.287801   \n",
       "\n",
       "       gene_6    gene_7    gene_8    gene_9  ...  gene_4991  gene_4992  \\\n",
       "0   -0.825144  0.023392  0.249881 -0.299325  ...  -0.303823  -1.997002   \n",
       "1    0.347168 -0.204902  0.481920 -0.362398  ...  -0.581623  -0.732298   \n",
       "2   -0.920093 -1.049402 -0.584051 -0.924871  ...   0.461320  -0.400628   \n",
       "3   -0.287336  0.016889  1.042713  0.395760  ...  -1.727101  -0.270288   \n",
       "4   -0.551130 -0.306343  1.384544  0.281415  ...   0.587151   1.441910   \n",
       "..        ...       ...       ...       ...  ...        ...        ...   \n",
       "995 -0.355663 -1.966713  0.788707  0.330298  ...   1.753508   1.066614   \n",
       "996  0.227806  0.710682  0.607903  0.902859  ...  -0.058202  -1.711519   \n",
       "997 -0.561344 -0.376735 -0.734039 -0.426135  ...  -0.151355  -1.563203   \n",
       "998 -0.371238  0.289356  2.841597  0.731189  ...   0.986182  -0.189626   \n",
       "999 -0.228249 -0.575495  0.448721  0.162182  ...  -1.376273  -0.178267   \n",
       "\n",
       "     gene_4993  gene_4994  gene_4995  gene_4996  gene_4997  gene_4998  \\\n",
       "0    -1.268515  -0.166643   0.173221   1.027971   0.299238  -0.077520   \n",
       "1    -1.348586   0.372561  -0.765171   0.190115  -0.048095  -0.098347   \n",
       "2     0.187157   0.267106   0.677449   1.074012   0.047437   0.935116   \n",
       "3     1.228877  -2.670715  -0.519727  -0.328582  -0.825309  -0.817777   \n",
       "4    -1.574249  -1.514545   0.830039  -0.379623   0.531506   0.906511   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "995   2.494430   1.558894   0.936826  -0.339432   1.196886   0.639897   \n",
       "996   0.063803   0.450900   1.855251   0.760492   0.570959  -0.437103   \n",
       "997   0.631759  -0.040236   1.014956  -0.695214   0.123247   0.391909   \n",
       "998   1.366594   0.174719   0.899885  -1.272737   0.970013  -0.255478   \n",
       "999   0.788993  -0.828447  -0.478302   0.499964   0.758177  -1.500024   \n",
       "\n",
       "     gene_4999  gene_5000  \n",
       "0     1.866126   0.217892  \n",
       "1    -0.277120  -0.048143  \n",
       "2    -0.628793  -0.764516  \n",
       "3    -0.716570  -0.224503  \n",
       "4     1.072289   0.666093  \n",
       "..         ...        ...  \n",
       "995   1.187620  -0.401922  \n",
       "996   0.205727  -0.674824  \n",
       "997   0.735752  -0.157513  \n",
       "998  -0.880389  -1.138162  \n",
       "999   0.600020  -0.108616  \n",
       "\n",
       "[1000 rows x 5001 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gene_data = pd.read_csv(\"data/gene expression.csv\")\n",
    "gene_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32994ffc",
   "metadata": {},
   "source": [
    "Poniewaz zmienna `disease_indicator` przyjmuje wartości z przestrzeni liczb rzeczywistych moglibyśmy użyć znanego nam już modelu regresji liniowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcead81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zadanie 1\n",
    "# Podziel zbiór na treningowy i testowy\n",
    "# Zbuduj model regresji liniowej\n",
    "# Podaj błąd kwadratowy na zbiorze treningowym i testowym\n",
    "# Podaj R^2 modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc17c74",
   "metadata": {},
   "source": [
    "Od razu widzimy, że coś jest nie tak. Na zbiorze treningowym osiaga prawie zerowy błąd, na zbiorze testowym jednak wypada on fatalnie. Jest to typowy przypkad **overfittingu**, czyli **zbyt wysokiej wariancji modelu**. Overfitting spowodowany jest uzyciem zbyt elastycznego modelu, jednakże zazwyczaj rozumiemy to poprzez użycie zbyt elastycznej architektury jak np. sieci nauronowego do prostego problemu. Model regresji liniowej jest bardzo prostym modelem więc nie powinno byc z tym problemu.\n",
    "\n",
    "Overfitting może jednak występować także dla prostych modeli w przypadku gdy posiadamy więcej predyktorów niż obserwacji: $p \\geq N$. We wspomianym przypadku estymatory oparte na metodzie największej wiarygodności jak i na metodzie najmniejszych kwadratów narażone sa na przeuczenie modelu. \n",
    "\n",
    "Zwizualizujmy co się dzieje na prostym przykładzie zakładając, że mamy w naszym zbiorze treningowym tylko 2 obserwacje i używamy jednego predyktora (dodając wyraz wolny mamy $p=N$). W takim przypadku łatwo zauważyć, że model regresji liniowej da nam po prostu wzór na linię przechodzącą przez obie obserwacje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2222)\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    \"x\": [1, 2, 3, 4, 5, 6, 7],\n",
    "    \"y\": [2*x + 1 + np.random.normal() for x in range(1, 8)],\n",
    "    \"set\": [\"train\", \"test\", \"train\", \"test\", \"test\", \"test\", \"test\"]\n",
    "})\n",
    "\n",
    "train_data = sample_data.loc[sample_data[\"set\"] == \"train\", [\"x\", \"y\"]]\n",
    "\n",
    "test_model = LinearRegression().fit(train_data[[\"x\"]], train_data[\"y\"])\n",
    "test_model_intercept = test_model.intercept_\n",
    "test_model_slope = test_model.coef_[0]\n",
    "\n",
    "sns.scatterplot(data=sample_data, x=\"x\", y=\"y\", hue=\"set\") \n",
    "plt.plot(sample_data[\"x\"], test_model_intercept + test_model_slope * sample_data[\"x\"], color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2bedb1",
   "metadata": {},
   "source": [
    "## Regresja grzbietowa\n",
    "\n",
    "Aby rozwiazać ten problem musimy zastanowić się jak zmnieszyć wariancję naszego modelu. Wariancja i obciążenie (bias) są ze sobą ścisle powiązane. Zmniejszająć jedno zwiększamy drugie - bias-variance tradeoff, tak więc moglibyśmy sprawdzić co stanie się gdy do naszego modelu dodamy obciażanie. dokonamy tego dodając **regularyzacji** naszego modelu, karając model za zbyt wysokie współczynniki $\\beta$. Mówiąc dokładniej w regularyzowanej wersji naszego modelu minimalizować będziemy nie loglikelihood (lub błąd kwadratowy), a poniższe wyrażenie:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = agrmin_{\\beta} (-\\ell(\\beta) + \\lambda\\sum_{i=1}^p \\beta_p^2)\n",
    "$$\n",
    "\n",
    "Jest to tak zwana **regresja grzbietowa** (Ridge regression). Parametr $\\lambda$ jest **hiperparametrem** modelu, który musimy ustalić. Jest to tak zwany **współczynnik kary** - im większa $\\lambda$ tym bardziej \"karamy\" nasz model za zbyt duże wartości parametrów. W przypadku regresji liniowej wartość likelihood mozemy zastąpić błędem kwadratowym, i bezpośrednio znaleźć postać rozwiązania:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^TX + \\lambda I)^{-1}X^Ty\n",
    "$$\n",
    "Zwizualizujmy rozwiązanie regresji grzbietowej dla kilku wartości $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(len(sample_data[sample_data[\"set\"] == \"train\"])), \n",
    "          sample_data[sample_data[\"set\"] == \"train\"][[\"x\"]].values]\n",
    "y = sample_data[sample_data[\"set\"] == \"train\"][[\"y\"]].values\n",
    "\n",
    "ridge_intercept = np.mean(y)\n",
    "\n",
    "solution_lambda_0 = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "solution_lambda_1 = np.linalg.inv(X.T @ X + 1*np.identity(X.shape[1])) @ X.T @ y\n",
    "solution_lambda_5 = np.linalg.inv(X.T @ X + 5*np.identity(X.shape[1])) @ X.T @ y\n",
    "solution_lambda_10 = np.linalg.inv(X.T @ X + 10*np.identity(X.shape[1])) @ X.T @ y\n",
    "\n",
    "sns.scatterplot(data=sample_data, x=\"x\", y=\"y\", hue=\"set\") \n",
    "plt.plot(sample_data[\"x\"], \n",
    "         test_model_intercept + test_model_slope * sample_data[\"x\"], \n",
    "         color='blue')\n",
    "plt.text(1.5, 14, \"lambda=0\", color=\"blue\")\n",
    "plt.plot(sample_data[\"x\"], \n",
    "         solution_lambda_1[0,0] + solution_lambda_1[1,0] * sample_data[\"x\"], \n",
    "         color='green')\n",
    "plt.text(1.5, 13, \"lambda=1\", color=\"green\")\n",
    "plt.plot(sample_data[\"x\"], \n",
    "         solution_lambda_5[0,0] + solution_lambda_5[1,0] * sample_data[\"x\"], \n",
    "         color='red')\n",
    "plt.text(1.5, 12, \"lambda=5\", color=\"red\")\n",
    "plt.plot(sample_data[\"x\"], \n",
    "         solution_lambda_10[0,0] + solution_lambda_10[1,0] * sample_data[\"x\"], \n",
    "         color='black')\n",
    "plt.text(1.5, 11, \"lambda=10\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76040861",
   "metadata": {},
   "source": [
    "Oczywiście pojawia się pytanie, jak wybrać wielkość $lambda$ ? Parametr ten wybierany jest w procesie **cross-walidacji** (CV). CV jest prostą lecz skuteczną metodą estymacji błędu generalizacji dlatego też doskonale nadaje się do estymacji wartości wszelkich hiperparametrów. \n",
    "\n",
    "W procesie CV dzielimy nasz zbiór treningowy na **foldy** równej wielkości. Nastepnie budujemy $K$ modeli za nowy zbiór treningowy biorąc $K-1$ foldów - w kazdym z modeli pozostający fold jest traktowany jako zbiór walidacyjny. Nastepnie uśredniamy wyniki (np. bład sredniokwadratowy) ze zbiorów walidacyjnych otrzymując estymację zbioru generalizacyjnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = gene_data.drop('disease_indicator', axis=1)\n",
    "y = gene_data['disease_indicator']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
    "\n",
    "param_grid = {'alpha': np.logspace(-4, 4, 9)}\n",
    "\n",
    "ridge = Ridge()\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "best_params = grid_search.best_params_\n",
    "coefs = []\n",
    "mse = []\n",
    "for a in param_grid['alpha']:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(X, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "    mse.append(np.mean((ridge.predict(X) - y) ** 2))\n",
    "\n",
    "print(f'Najlepsza wartość lambda: {best_params[\"alpha\"]}')\n",
    "print(f'Wynik regresji: {grid_search.best_score_}')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(param_grid['alpha'], mse)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE w zależności od Lambda')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(param_grid['alpha'], coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Współczynniki')\n",
    "plt.title('Współczynniki Ridge w zależności od Lambda')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed6406",
   "metadata": {},
   "source": [
    "Mozy teraz policzyć predykcje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = grid_search.predict(X_train)\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Błąd kwadratowy dla zbioru treningowego: \", mse_train)\n",
    "print(\"Błąd kwadratowy dla zbioru testowego: \", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc2873",
   "metadata": {},
   "source": [
    "Regresja grzbietowa jest przykładem **regularyzacji normą $L^q$** gdzie $q=2$:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = agrmin_{\\beta} (-\\ell(\\beta) + \\lambda\\sum_{i=1}^p |\\beta_p|^q)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f840020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_norm(x, q):\n",
    "    return (1 - abs(x ** q)) ** (1/q)\n",
    "\n",
    "q_norms = pd.DataFrame()\n",
    "q_values = [0.3, 0.5, 1, 2, 3, 5, 20]\n",
    "\n",
    "for q in q_values:\n",
    "    points = pd.DataFrame({\n",
    "        'x': np.arange(0, 1.001, 0.001),\n",
    "        'y': gen_norm(np.arange(0, 1.001, 0.001), q),\n",
    "        'q': q\n",
    "    })\n",
    "    points_1 = points.copy()\n",
    "    points_2 = points.copy()\n",
    "    points_3 = points.copy()\n",
    "    points_4 = points.copy()\n",
    "    points_1['y'] = -points_1['y']\n",
    "    points_2['x'] = -points_2['x']\n",
    "    points_3['x'] = -points_3['x']\n",
    "    points_3['y'] = -points_3['y']\n",
    "    q_norms = pd.concat([q_norms, points, points_1, points_2, points_3, points_4])\n",
    "\n",
    "q_norms['q'] = q_norms['q'].astype('category')\n",
    "q_norms = q_norms.sort_values(['q', 'y', 'x'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(q_norms['x'], q_norms['y'], c=q_norms['q'], cmap='viridis', alpha=0.8, s=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Kula w przestrzenie L^q')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b0b97",
   "metadata": {},
   "source": [
    "## Regresja LASSO\n",
    "\n",
    "Moznaby się zastanowić co stanie się gdy użyjemy innej normy do regularyzacji. Najczęsciej spotykanym kuzynem regresji grzbietowej jest **regresja LASSO** (least absolute shrinkage and selection operator), gdzie $q=1$:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = agrmin_{\\beta} (-\\ell(\\beta) + \\lambda\\sum_{i=1}^p |\\beta_p|)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca75dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = gene_data.drop('disease_indicator', axis=1)\n",
    "y = gene_data['disease_indicator']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
    "\n",
    "param_grid = {'alpha': np.logspace(-4, 4, 9)}\n",
    "\n",
    "lasso = Lasso()\n",
    "grid_search = GridSearchCV(estimator=lasso, param_grid=param_grid, cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "best_params = grid_search.best_params_\n",
    "coefs = []\n",
    "mse = []\n",
    "for a in param_grid['alpha']:\n",
    "    lasso = Lasso(alpha=a)\n",
    "    lasso.fit(X, y)\n",
    "    coefs.append(lasso.coef_)\n",
    "    mse.append(np.mean((lasso.predict(X) - y) ** 2))\n",
    "\n",
    "print(f'Najlepsza wartość lambda: {best_params[\"alpha\"]}')\n",
    "print(f'Wynik regresji: {grid_search.best_score_}')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(param_grid['alpha'], mse)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE w zależności od Lambda')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(param_grid['alpha'], coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Współczynniki')\n",
    "plt.title('Współczynniki Ridge w zależności od Lambda')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = grid_search.predict(X_train)\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Błąd kwadratowy dla zbioru treningowego: \", mse_train)\n",
    "print(\"Błąd kwadratowy dla zbioru testowego: \", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973227df",
   "metadata": {},
   "source": [
    "Widać tu pewną różnicę jesli chodzi o wartości współczynników $\\beta$. W regresji grzbietowej niektóre wartości są bliskie 0, ale nigdy nie osiągają tej wartości - metoda regresji grzbietowej pozwala tylko na dojście asymptotycznie blisko do 0. W przypadku LASSO dostajemy zerowe współczynniki - daje nam to bardzo przydatną własność, bo LASSO działa jak **selektor zmiennych**.\n",
    "\n",
    "## Regresja Elastic Net\n",
    "\n",
    "LASSO daje z reguły lepsze wyniki gdy w naszym zbiorze mamy nieistotne predyktory (zmienne które nie wnoszą informacji do modelu), LASSO potrafi się ich pozbyć. Regresja grzbietowa działa lepiej gdy predyktory są istotne. Jeśli chcemy korzystać z zalet obu podejść możemy użyć regresji **Elastic Net**:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = agrmin_{\\beta} (-\\ell(\\beta) + \\lambda(\\alpha\\sum_{i=1}^p |\\beta_p| + (1-\\alpha)\\sum_{i=1}^p \\beta_p^2))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = gene_data.drop('disease_indicator', axis=1)\n",
    "y = gene_data['disease_indicator']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)\n",
    "\n",
    "param_grid = {'alpha': np.logspace(-4, 4, 9)}\n",
    "\n",
    "elastic_net = ElasticNet(l1_ratio=0.5)\n",
    "grid_search = GridSearchCV(estimator=elastic_net, param_grid=param_grid, cv=10)\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "best_params = grid_search.best_params_\n",
    "coefs = []\n",
    "mse = []\n",
    "for a in param_grid['alpha']:\n",
    "    elastic_net = ElasticNet(alpha=a, l1_ratio=0.5)\n",
    "    elastic_net.fit(X, y)\n",
    "    coefs.append(elastic_net.coef_)\n",
    "    mse.append(np.mean((elastic_net.predict(X) - y) ** 2))\n",
    "\n",
    "print(f'Najlepsza wartość lambda: {best_params[\"alpha\"]}')\n",
    "print(f'Wynik regresji: {grid_search.best_score_}')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(param_grid['alpha'], mse)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE w zależności od Lambda')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(param_grid['alpha'], coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Współczynniki')\n",
    "plt.title('Współczynniki Ridge w zależności od Lambda')\n",
    "plt.axis('tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b2767",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = grid_search.predict(X_train)\n",
    "y_test_pred = grid_search.predict(X_test)\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Błąd kwadratowy dla zbioru treningowego: \", mse_train)\n",
    "print(\"Błąd kwadratowy dla zbioru testowego: \", mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fb481",
   "metadata": {},
   "source": [
    "Wartość $\\alpha$ jest kolejnym hiperparametrem który należałoby wybrać z użyciem CV i metody seleckji jak np grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1de4fc",
   "metadata": {},
   "source": [
    "# Zadanie\n",
    "Zbuduj model regresji logistycznej z regularyzacją dla poniższego zbioru danych.\n",
    "Sprawdź braki danych, rozkłady zmiennych, użyj modułu scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54792266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "filepaths = os.listdir(\"data/creditcard/\")\n",
    "for fp in filepaths:\n",
    "    with open(\"data/creditcard/\" + fp, 'rb') as f:\n",
    "        globals()[fp.replace(\".pickle\", \"\")] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d2a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
